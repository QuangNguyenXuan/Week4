{"cells":[{"cell_type":"markdown","source":["## **0. Tải bộ dữ liệu**\n","**Lưu ý:** Nếu không thể tải bằng gdown do bị giới hạn số lượt tải, các bạn hãy tải thủ công và đưa lên drive của mình, sau đó copy từ drive vào colab.\n","```python\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!cp /path/to/dataset/on/your/drive .\n","```"],"metadata":{"id":"EG4pEApweeMi"}},{"cell_type":"code","source":["# https://drive.google.com/file/d/15q2J4DlwDUrTqKUUpClcwc2HaXO_tWCY/view?usp=share_link\n","!gdown --id 15q2J4DlwDUrTqKUUpClcwc2HaXO_tWCY"],"metadata":{"id":"V2H_gNugelj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"U1NHytiqPTr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/drive/MyDrive/Coordinate/aio_2022_ta/module_7/image_captioning/dataset/space_image_captioning_dataset_100.zip' ."],"metadata":{"id":"bWdtL4zXPkK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip './space_image_captioning_dataset_100.zip'"],"metadata":{"id":"7RHFkwgge2Nj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **1. Import các thư viện cần thiết**"],"metadata":{"id":"TSsat2oUaitv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x753CoSmVZ1G"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import re\n","import random\n","\n","from PIL import Image\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","\n","RANDOM_SEED = 1\n","np.random.seed(RANDOM_SEED)\n","tf.random.set_seed(RANDOM_SEED)"]},{"cell_type":"markdown","source":["## **2. Chuẩn bị dữ liệu**"],"metadata":{"id":"YKb5snF9aptz"}},{"cell_type":"code","source":["def text_normalize(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    text = re.sub('\\s+', ' ', text)\n","    text = text.strip() \n","    text = '<start> ' + text + ' <end>'\n"," \n","    return text"],"metadata":{"id":"sfZ3eP0kPsi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","MAX_SEQ_LEN = 30\n","IMG_SIZE = (128, 128)\n","VOCAB_SIZE = 5000 \n","AUTOTUNE = tf.data.AUTOTUNE"],"metadata":{"id":"rxbeWccEX4RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_ROOT_DIR = './space_image_captioning_dataset'\n","IMAGES_DIR_NAME = 'space_images'\n","CAPTIONS_FILENAME = 'space_captions.txt'\n","\n","images_dir = os.path.join(\n","    DATASET_ROOT_DIR, \n","    IMAGES_DIR_NAME\n",")\n","\n","captions_filepath = os.path.join(\n","    DATASET_ROOT_DIR, \n","    CAPTIONS_FILENAME\n",")"],"metadata":{"id":"gJQ9o5gQANNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(captions_filepath, 'r', encoding='utf8') as f:\n","    lines = f.readlines()\n","    dataset_dict = {}\n","    skipped_images_lst = []\n","    corpus = []\n","    for line in lines:\n","        line = line.rstrip('\\n')\n","        img_path, caption = line.split('\\t')\n","\n","        tokens = caption.split(' ')\n","\n","        if len(tokens) < 5 or len(tokens) > (MAX_SEQ_LEN - 2):\n","            skipped_images_lst.append(img_path)\n","            continue\n","        else:\n","            normalized_caption = text_normalize(caption)\n","            corpus.append(normalized_caption)\n","            dataset_dict[img_path] = normalized_caption        "],"metadata":{"id":"YjSMjEex0gOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["items_to_show = [\n","    (img_path, caption) \n","    for idx, (img_path, caption) in \\\n","        enumerate(dataset_dict.items()) if idx < 20\n","]\n","\n","for idx in range(10):\n","    img_filename, caption = items_to_show[idx]\n","    img_path = os.path.join(images_dir, img_filename.split('/')[-1])\n","    img = Image.open(img_path)\n","    plt.imshow(img)\n","    plt.title(f'Caption: {caption}', loc='left', y=-0.1)\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"b68VnUSn8MMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode=\"int\",\n","    output_sequence_length=MAX_SEQ_LEN,\n","    standardize=None,\n",")\n","vectorizer.adapt(corpus)\n","\n","VOCAB_SIZE = int(vectorizer.vocabulary_size())\n","\n","img_augmentation = tf.keras.Sequential(\n","    [\n","        tf.keras.layers.RandomFlip(\"horizontal\"),\n","        tf.keras.layers.RandomRotation(0.2),\n","        tf.keras.layers.RandomContrast(0.3),\n","    ]\n",")"],"metadata":{"id":"IL4WgUmxYNHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_img(img_path):\n","    img = tf.io.read_file(img_path) \n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, IMG_SIZE)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","\n","    return img\n","\n","def process_input_data(img_path, caption, augment=False):\n","    img = preprocess_img(img_path)\n","    if augment == True:\n","        img = img_augmentation(img)\n","    cap = vectorizer(caption) \n","    decoder_input = cap[:-1]\n","    decoder_output = cap[1:]\n","\n","    return (img, decoder_input), decoder_output"],"metadata":{"id":"lZNH99JVV0hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array(list(dataset_dict.keys()))\n","y = np.array(list(dataset_dict.values()))\n","\n","TRAIN_SIZE = 0.7\n","VAL_SIZE = 0.2\n","n_samples = len(X)\n","train_idx = int(n_samples * TRAIN_SIZE)\n","val_idx = train_idx + int(n_samples * VAL_SIZE)\n","\n","\n","idxs = np.arange(n_samples)\n","idxs = np.random.permutation(idxs)\n","\n","X = X[idxs]\n","y = y[idxs]\n","\n","X_train, y_train = X[:train_idx], y[:train_idx]\n","X_val, y_val = X[train_idx:val_idx], y[train_idx:val_idx] \n","X_test, y_test = X[val_idx:], y[val_idx:]"],"metadata":{"id":"xGmWikjMS1ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = tf.data.Dataset.from_tensor_slices(\n","    (X_train, y_train)\n",")\n","\n","val_ds = tf.data.Dataset.from_tensor_slices(\n","    (X_val, y_val)\n",")\n","\n","test_ds = tf.data.Dataset.from_tensor_slices(\n","    (X_test, y_test)\n",")\n","\n","train_ds = train_ds.cache().prefetch(\n","    buffer_size=AUTOTUNE\n",").map(\n","    lambda x, y: process_input_data(x, y, augment=True), \n","    num_parallel_calls=AUTOTUNE\n",").batch(BATCH_SIZE)\n","\n","val_ds = val_ds.cache().prefetch(\n","    buffer_size=AUTOTUNE\n",").map(\n","    process_input_data, \n","    num_parallel_calls=AUTOTUNE\n",").batch(BATCH_SIZE)\n","\n","test_ds = test_ds.cache().prefetch(\n","    buffer_size=AUTOTUNE\n",").map(\n","    process_input_data, \n","    num_parallel_calls=AUTOTUNE\n",").batch(BATCH_SIZE)"],"metadata":{"id":"onM2jAEBte9v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Xây dựng mô hình**"],"metadata":{"id":"iBYXRSW8at00"}},{"cell_type":"markdown","source":["### 3.1. Positional Encoding Layer"],"metadata":{"id":"eVEKM0CM3Awi"}},{"cell_type":"code","source":["def positional_encoding(length, depth):\n","    depth = depth/2\n","\n","    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","    angle_rates = 1 / (10000**depths)         # (1, depth)\n","    angle_rads = positions * angle_rates      # (pos, depth)\n","\n","    pos_encoding = np.concatenate(\n","        [np.sin(angle_rads), np.cos(angle_rads)],\n","        axis=-1) \n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, d_model):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n","        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n","\n","    def compute_mask(self, *args, **kwargs):\n","        return self.embedding.compute_mask(*args, **kwargs)\n","\n","    def call(self, x):\n","        length = tf.shape(x)[1]\n","        x = self.embedding(x)\n","        # This factor sets the relative scale of the embedding and positonal_encoding.\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x"],"metadata":{"id":"nEFbBsLs2q95"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2. Self-attention Layer"],"metadata":{"id":"_kprsBbOZuEo"}},{"cell_type":"code","source":["class BaseAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","        self.layernorm = tf.keras.layers.LayerNormalization()\n","        self.add = tf.keras.layers.Add()"],"metadata":{"id":"briHZ9rSZnVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3. Cross-attention Layer"],"metadata":{"id":"aw78qUVVZzRB"}},{"cell_type":"code","source":["class CrossAttention(BaseAttention):\n","    def call(self, x, context):\n","        attn_output, attn_scores = self.mha(\n","            query=x,\n","            key=context,\n","            value=context,\n","            return_attention_scores=True)\n","\n","        # Cache the attention scores for plotting later.\n","        self.last_attn_scores = attn_scores\n","\n","        x = self.add([x, attn_output])\n","        x = self.layernorm(x)\n","\n","        return x"],"metadata":{"id":"5A7NAx-BZ1co"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.4. Global Self-attention Layer"],"metadata":{"id":"M8jxIcNeqTFc"}},{"cell_type":"code","source":["class GlobalSelfAttention(BaseAttention):\n","    def call(self, x):\n","        attn_output = self.mha(\n","            query=x,\n","            value=x,\n","            key=x)\n","        x = self.add([x, attn_output])\n","        x = self.layernorm(x)\n","        \n","        return x"],"metadata":{"id":"na8ia9lrqScK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.5. Causal Self-attention Layer"],"metadata":{"id":"-gMM5fDMqUTN"}},{"cell_type":"code","source":["class CausalSelfAttention(BaseAttention):\n","    def call(self, x):\n","        attn_output = self.mha(\n","            query=x,\n","            value=x,\n","            key=x,\n","            use_causal_mask = True)\n","        x = self.add([x, attn_output])\n","        x = self.layernorm(x)\n","        \n","        return x"],"metadata":{"id":"YTpvCb98qUXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.6. Feed-forward Network"],"metadata":{"id":"qwTer3wbqY5R"}},{"cell_type":"code","source":["class FeedForward(tf.keras.layers.Layer):\n","    def __init__(self, d_model, dff, dropout_rate=0.1):\n","        super().__init__()\n","        self.seq = tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model),\n","        tf.keras.layers.Dropout(dropout_rate)\n","        ])\n","        self.add = tf.keras.layers.Add()\n","        self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","    def call(self, x):\n","        x = self.add([x, self.seq(x)])\n","        x = self.layer_norm(x) \n","        return x"],"metadata":{"id":"iSvvJ4RIqZC6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.7. Encoder Layer"],"metadata":{"id":"1ooM5G4zqdUX"}},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.self_attention = GlobalSelfAttention(\n","            num_heads=num_heads,\n","            key_dim=d_model,\n","            dropout=dropout_rate)\n","\n","        self.ffn = FeedForward(d_model, dff)\n","\n","    def call(self, x):\n","        x = self.self_attention(x)\n","        x = self.ffn(x)\n","        return x"],"metadata":{"id":"2Rs7SIkBqdhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.8. Transformer Encoder"],"metadata":{"id":"CuxixKozqgsn"}},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, *, num_layers, d_model, num_heads,\n","                dff, vocab_size, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.pos_embedding = PositionalEmbedding(\n","            vocab_size=vocab_size, d_model=d_model)\n","\n","        self.enc_layers = [\n","            EncoderLayer(d_model=d_model,\n","                        num_heads=num_heads,\n","                        dff=dff,\n","                        dropout_rate=dropout_rate)\n","            for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","        self.layernorm = tf.keras.layers.LayerNormalization()\n","        self.ffn = tf.keras.layers.Dense(d_model, activation='relu') \n","\n","    def call(self, x):\n","        x = self.layernorm(x)\n","        x = self.ffn(x)\n","\n","        # Add dropout.\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x)\n","\n","        return x  # Shape `(batch_size, seq_len, d_model)`."],"metadata":{"id":"ecVPFOKdqivK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.9. Decoder Layer"],"metadata":{"id":"wQNF02D4qmiu"}},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self,\n","                *,\n","                d_model,\n","                num_heads,\n","                dff,\n","                dropout_rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.causal_self_attention = CausalSelfAttention(\n","            num_heads=num_heads,\n","            key_dim=d_model,\n","            dropout=dropout_rate)\n","\n","        self.cross_attention = CrossAttention(\n","            num_heads=num_heads,\n","            key_dim=d_model,\n","            dropout=dropout_rate)\n","\n","        self.ffn = FeedForward(d_model, dff)\n","\n","    def call(self, x, context):\n","        x = self.causal_self_attention(x=x)\n","        x = self.cross_attention(x=x, context=context)\n","\n","        # Cache the last attention scores for plotting later\n","        self.last_attn_scores = self.cross_attention.last_attn_scores\n","\n","        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n","        return x"],"metadata":{"id":"Kmp3hlIiqmnG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.10. Transformer Decoder"],"metadata":{"id":"UqHThZq4qpzH"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n","                dropout_rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n","                                                d_model=d_model)\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","        self.dec_layers = [\n","            DecoderLayer(d_model=d_model, num_heads=num_heads,\n","                        dff=dff, dropout_rate=dropout_rate)\n","            for _ in range(num_layers)]\n","\n","        self.last_attn_scores = None\n","\n","    def call(self, x, context):\n","        # `x` is token-IDs shape (batch, target_seq_len)\n","        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n","\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x  = self.dec_layers[i](x, context)\n","\n","        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n","\n","        # The shape of x is (batch_size, target_seq_len, d_model).\n","        return x"],"metadata":{"id":"BSHMjfJ0qpbV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.11. CNN Model"],"metadata":{"id":"0fP2OnV6aozo"}},{"cell_type":"code","source":["def get_cnn_model():    \n","    # Option 1: Freeze layers\n","    base_model = tf.keras.applications.vgg16.VGG16(\n","        input_shape=(*IMG_SIZE, 3), \n","        include_top=False, \n","        weights=\"imagenet\"\n","    )\n","    base_model.trainable = False\n","\n","    ## Option 2: Initialize new weights and unfreeze layers\n","    ## Uncomment these and replace to option 1\n","    # base_model = tf.keras.applications.vgg16.VGG16(\n","    #     input_shape=(*IMG_SIZE, 3), \n","    #     include_top=False, \n","    #     weights=None\n","    # )\n","\n","    # for layer in base_model.layers:\n","    #     if hasattr(layer, 'kernel_initializer'):\n","    #         layer.kernel_initializer = tf.keras.initializers.HeNormal()\n","    # base_model.trainable = True\n","\n","    base_model_out = base_model.output\n","    base_model_out = tf.keras.layers.Reshape(\n","        (-1, base_model_out.shape[-1])\n","    )(base_model_out)\n","    cnn_model = tf.keras.models.Model(\n","        base_model.input, \n","        base_model_out\n","    )\n","\n","    return cnn_model"],"metadata":{"id":"z87DQ1O6aqsf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.12. Image Captioning Model"],"metadata":{"id":"4WHwb3e3-3ds"}},{"cell_type":"code","source":["class ImageCaptioningModel(tf.keras.Model):\n","    def __init__(self, *, num_layers, d_model, num_heads, dff,\n","                vocab_size, dropout_rate=0.1):\n","        super().__init__()\n","        self.cnn_model = get_cnn_model()\n","        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n","                            num_heads=num_heads, dff=dff,\n","                            vocab_size=vocab_size,\n","                            dropout_rate=dropout_rate)\n","\n","        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n","                            num_heads=num_heads, dff=dff,\n","                            vocab_size=vocab_size,\n","                            dropout_rate=dropout_rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, inputs):\n","        # To use a Keras model with `.fit` you must pass all your inputs in the\n","        # first argument.\n","        img, x  = inputs\n","\n","        context = self.cnn_model(img)\n","\n","        context = self.encoder(context)  # (batch_size, context_len, d_model)\n","\n","        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n","\n","        # Final linear layer output.\n","        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n","\n","        try:\n","            # Drop the keras mask, so it doesn't scale the losses/metrics.\n","            # b/250038731\n","            del logits._keras_mask\n","        except AttributeError:\n","            pass\n","\n","        # Return the final output and the attention weights.\n","        return logits"],"metadata":{"id":"bc8lJ0g5-5n7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_LAYERS = 4\n","D_MODEL = 128\n","D_FF = 512\n","N_HEADS = 8\n","DROPOUT_RATE = 0.2"],"metadata":{"id":"45nTAWsWPSks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ic_model = ImageCaptioningModel(\n","    num_layers=N_LAYERS,\n","    d_model=D_MODEL,\n","    num_heads=N_HEADS,\n","    dff=D_FF,\n","    vocab_size=VOCAB_SIZE,\n","    dropout_rate=DROPOUT_RATE\n",")"],"metadata":{"id":"8-R7Z-7hP_em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in val_ds.take(1):\n","    a, b = batch"],"metadata":{"id":"9viLFa6o_0Oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = ic_model.predict(a, verbose=0)"],"metadata":{"id":"2E-bdJEteFOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out.shape"],"metadata":{"id":"gKD0ODRvtoOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ic_model.summary()"],"metadata":{"id":"hvkV3HDv_IuC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4. Cấu hình mô hình**"],"metadata":{"id":"YLf9DMbmbX9M"}},{"cell_type":"code","source":["# Khai báo một số giá trị siêu tham số\n","EPOCHS = 50"],"metadata":{"id":"RVY1Eo7wxmbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, dtype=tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"v-FdNRo6_U_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, \n","    beta_1=0.9, \n","    beta_2=0.98,\n","    epsilon=1e-9\n",")"],"metadata":{"id":"ed4dHZkf_XC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def masked_loss(label, pred):\n","    mask = label != 0\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","    loss = loss_object(label, pred)\n","\n","    mask = tf.cast(mask, dtype=loss.dtype)\n","    loss *= mask\n","\n","    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","\n","    return loss\n","\n","\n","def masked_accuracy(label, pred):\n","    pred = tf.argmax(pred, axis=2)\n","    label = tf.cast(label, pred.dtype)\n","    match = label == pred\n","\n","    mask = label != 0\n","\n","    match = match & mask\n","\n","    match = tf.cast(match, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","    \n","    return tf.reduce_sum(match)/tf.reduce_sum(mask)"],"metadata":{"id":"Ni0iuAHj_hLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ic_model.compile(\n","    loss=masked_loss,\n","    optimizer=optimizer,\n","    metrics=[masked_accuracy]\n",")"],"metadata":{"id":"cR868LBvxoZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5. Thực hiện huấn luyện**"],"metadata":{"id":"3Kb90Safbc0I"}},{"cell_type":"code","source":["history = ic_model.fit(\n","    train_ds,\n","    epochs=EPOCHS,\n","    validation_data=val_ds\n",")"],"metadata":{"id":"-StpXxgExtFo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6. Đánh giá và trực quan hóa**"],"metadata":{"id":"UgNNy8JUbfBj"}},{"cell_type":"code","source":["# Đánh giá mô hình trên tập test\n","test_evaluation = ic_model.evaluate(test_ds)"],"metadata":{"id":"b3TlfKI5rc7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Đọc các kết quả huấn luyện mô hình qua từng epoch\n","train_loss, train_acc = history.history['loss'], history.history['masked_accuracy'] # Đọc thông tin loss, acc trên tập train\n","val_loss, val_acc = history.history['val_loss'], history.history['val_masked_accuracy'] # Đọc thông tin loss, acc trên tập val\n","\n","plt.figure(figsize=(10, 10)) # Cài đặt kích thước khung ảnh\n","\n","plt.subplot(2, 2, 1) # Khởi tạo khung ảnh cho training loss\n","plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n","plt.ylabel('Loss') # Hiển thị tên trục tung là 'Loss'\n","plt.title('Training loss') # Hiển thị title của khung ảnh hiện tại là 'Training Loss'\n","plt.plot(train_loss, color='green') # Vẽ đường giá trị loss trên tập train qua từng epoch (đường vẽ màu đỏ)\n","\n","plt.subplot(2, 2, 2) # Khởi tạo khung ảnh cho training acc\n","plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n","plt.ylabel('Accuracy') # Hiển thị tên trục tung là 'Accuracy'\n","plt.title('Training accuracy') # Hiển thị title của khung ảnh hiện tại là 'Training accuracy'\n","plt.plot(train_acc, color='orange') # Vẽ đường giá trị accuracy trên tập train qua từng epoch (đường vẽ màu cam)\n","\n","plt.subplot(2, 2, 3) # Khởi tạo khung ảnh cho val loss\n","plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n","plt.ylabel('Loss') # Hiển thị tên trục tung là 'Loss'\n","plt.title('Validation loss') # Hiển thị title của khung ảnh hiện tại là 'Validation loss'\n","plt.plot(val_loss, color='green') # Vẽ đường giá trị loss trên tập val qua từng epoch (đường vẽ màu đỏ)\n","\n","plt.subplot(2, 2, 4) # Khởi tạo khung ảnh cho val acc\n","plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n","plt.ylabel('Accuracy') # Hiển thị tên trục tung là 'Accuracy'\n","plt.title('Validation accuracy') # Hiển thị title của khung ảnh hiện tại là 'Validation accuracy'\n","plt.plot(val_acc, color='orange') # Vẽ đường giá trị accuracy trên tập val qua từng epoch (đường vẽ màu cam)\n","\n","plt.show() # Hiển thị 4 khung ảnh nhỏ"],"metadata":{"id":"vmMe97kpcS71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = vectorizer.get_vocabulary()\n","index_lookup = dict(zip(range(len(vocab)), vocab))"],"metadata":{"id":"rBzrwkMYACh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_perplexity(logits, targets):\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","    loss = loss_fn(targets, logits)\n","\n","    perplexity = np.exp(np.mean(loss))\n","\n","    return perplexity\n","\n","\n","def compute_bleu(predicted, targets):\n","    predicted_strings = []\n","    for seq in predicted:\n","        seq = np.argmax(seq, axis=1)\n","        string_seq = \" \".join([index_lookup[token] for token in seq if token != 0])\n","        predicted_strings.append(string_seq)\n","    target_strings = []\n","    for seq in targets:\n","        seq = seq.numpy().tolist()\n","        string_seq = \" \".join([index_lookup[token] for token in seq if token != 0])\n","        target_strings.append([string_seq])  \n","\n","    bleu_score = corpus_bleu(target_strings, predicted_strings)\n","\n","    return bleu_score\n","\n","perplexities = []\n","bleu_scores = []\n","\n","for sample in test_ds.take(10):\n","    X_try, y_try = sample\n","    y_pred = ic_model(X_try)\n","\n","    batch_perplexity = compute_perplexity(y_pred, y_try)\n","    batch_bleu_score = compute_bleu(y_pred, y_try)\n","\n","    perplexities.append(batch_perplexity)\n","    bleu_scores.append(batch_bleu_score)\n","\n","    # print(f'Perplexity: {batch_perplexity}')\n","    # print(f'BLEU score: {batch_bleu_score}')\n","\n","print(f'Perplexity: {sum(perplexities) / len(perplexities)}')\n","print(f'BLEU score: {sum(bleu_scores) / len(bleu_scores)}')"],"metadata":{"id":"OMO_szhoIxcq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **7. Inference**"],"metadata":{"id":"8pSQL9BO0sry"}},{"cell_type":"code","source":["def generate_caption(\n","    img_path,\n","    ic_model,\n","    vectorizer\n","):\n","    img = preprocess_img(img_path)\n","    display_img = img.numpy().clip(0, 255).astype(np.uint8)\n","    \n","    img = np.expand_dims(display_img, axis=0)\n","    output_str = '<start>'\n","\n","    for idx in range(MAX_SEQ_LEN):\n","        tokenized_caption = vectorizer([output_str])[:, :-1]\n","        pred = ic_model.predict((img, tokenized_caption), verbose=0)\n","        sampled_token_idx = np.argmax(pred[0, idx, :])\n","        sampled_token = index_lookup[sampled_token_idx]\n","        if sampled_token == '<end>':\n","            break \n","        output_str += f' {sampled_token}'\n","    \n","    generated_caption = output_str.replace('<start>', '')\n","    generated_caption = generated_caption.replace('<end>', '')\n","    generated_caption = generated_caption.strip()\n","\n","    return display_img, generated_caption"],"metadata":{"id":"d6yLcZLB6QFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_img_path = '/content/space_image_captioning_dataset/space_images/IMG_000016.jpg'\n","display_img, generated_caption = generate_caption(\n","    input_img_path,\n","    ic_model,\n","    vectorizer\n",")\n","\n","plt.imshow(display_img)\n","plt.title(f'Caption: {generated_caption}', loc='left', y=-0.1)\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"MkCZnhJGZGws"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}